{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cell - imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import config\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Run this cell first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Second cell - initialize driver\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third cell - login\n",
    "def login():\n",
    "    driver.get(\"https://www.instagram.com\")\n",
    "    \n",
    "    username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='username']\")))\n",
    "    password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='password']\")))\n",
    "    \n",
    "    username.clear()\n",
    "    username.send_keys(config.username)\n",
    "    password.clear()\n",
    "    password.send_keys(config.password)\n",
    "    \n",
    "    button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\")))\n",
    "    button.click()\n",
    "\n",
    "# Run login when ready\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth cell - search function\n",
    "def search(query):\n",
    "    search_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"svg[aria-label='Search']\")))\n",
    "    search_button.click()\n",
    "    \n",
    "    search_input = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[placeholder='Search']\"))\n",
    "    )\n",
    "    \n",
    "    search_input.clear()\n",
    "    search_input.send_keys(query)\n",
    "    search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "# Try different searches\n",
    "keyword = \"sandyi01314\"\n",
    "search(keyword)  # You can change this and run the cell multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth cell - click on the firstsearch result\n",
    "\n",
    "#remove the @ symbol if present \n",
    "if keyword.startswith(\"@\"):\n",
    "    keyword = keyword[1:]\n",
    "\n",
    "try:\n",
    "    # Find the span with exact text content and click its parent link\n",
    "    clickable_element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, f\"//span[text()='{keyword}']/ancestor::a\"))\n",
    "    )\n",
    "    driver.execute_script(\"arguments[0].click();\", clickable_element)\n",
    "except TimeoutException:\n",
    "    print(f\"Could not click on result for '{keyword}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sixth cell - scroll loop\n",
    "\n",
    "#Get the initial page height\n",
    "initial_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "#create a list to store htmls\n",
    "soups = []\n",
    "\n",
    "#scroll loop\n",
    "while True:\n",
    "    #scroll to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #wait for the page to load\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #Parse the HTML\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Create a BeautilfulSoup object from scraped HTML\n",
    "    soups.append(BeautifulSoup(html, 'html.parser'))\n",
    "\n",
    "    #Get the current page height\n",
    "    current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    if current_height == initial_height:\n",
    "        break   #exit the loop when you can't scroll further\n",
    "\n",
    "    # Update the initial height for the next iteration\n",
    "    initial_height = current_height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seventh cell - extract all the urls of the posts\n",
    "\n",
    "#list to store the post image urls\n",
    "post_urls = []\n",
    "i = 0\n",
    "\n",
    "# loop through soup elements\n",
    "for soup in soups:\n",
    "    i+=1\n",
    "    print(f\"Processing soup {i} of {len(soups)}\")\n",
    "\n",
    "    #find all image elements that match the specific class in the current soup\n",
    "    elements = soup.find_all('a', class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz _a6hd\")\n",
    "\n",
    "    #Extract the href attributes and filter URLs that start with \"/p/\" or \"/reel/\"\n",
    "    post_urls.extend([element['href'] for element in elements if '/p/' in element['href'] or '/p/' in element['href']])\n",
    "\n",
    "#conver the list to a set to remove duplicates\n",
    "unique_post_urls = list(set(post_urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_post_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list to store the json for each post\n",
    "json_list =[]\n",
    "\n",
    "# Define the query parameters to add\n",
    "query_params = \"__a=1&__d=dis\"\n",
    "\n",
    "#go through all urls\n",
    "for url in unique_post_urls:\n",
    "\n",
    "    #Error handling\n",
    "    try:\n",
    "\n",
    "        #Get the current URL of the page\n",
    "        current_url = driver.current_url\n",
    "        \n",
    "        #Append the query parameters to the URL\n",
    "        modified_url = \"https://www.instagram.com\" + url + \"?\" + query_params\n",
    "\n",
    "        #get URL\n",
    "        driver.get(modified_url)\n",
    "\n",
    "        #wait for a moment to allow new content to load (adjust as needed)\n",
    "        time.sleep(1)\n",
    "\n",
    "        #Find the <pre> tag containing the JSON data\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//pre\"))\n",
    "        )\n",
    "        pre_tag = driver.find_element(By.XPATH, '//pre')  # Updated this line\n",
    "\n",
    "        #Extract the JSON data from the <pre> tag\n",
    "        json_script = pre_tag.text\n",
    "        \n",
    "        #Parse the JSON data\n",
    "        json_parsed = json.loads(json_script)\n",
    "\n",
    "        #Add json to the list\n",
    "        json_list.append(json_parsed)\n",
    "\n",
    "    except (NoSuchElementException, TimeoutException, json.JS) as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists to store URLs and corresponding dates\n",
    "all_urls = []\n",
    "all_dates = []\n",
    "\n",
    "#iterate through each JSON data in the list\n",
    "for json_data in json_list:\n",
    "\n",
    "    #Extract the list from the 'items' key\n",
    "    item_list = json_data.get('items',[])\n",
    "\n",
    "    #iterate through each item in 'items' list\n",
    "    for item in item_list:\n",
    "\n",
    "        #extract the date the item was taken\n",
    "        date_taken = item.get('taken_at')\n",
    "\n",
    "        #check if the carousel media is present\n",
    "        carousel_media = item.get('carousel_media',[])\n",
    "\n",
    "        #iterate through each item in the 'carousel_media' list\n",
    "        for media in carousel_media:\n",
    "\n",
    "            #extract the image url from the media\n",
    "            image_url = media.get('image_versions2', {}).get('candidates', [])[0].get('url')\n",
    "\n",
    "            #check if the image_url field is found inside the 'carousel_media' list\n",
    "            if image_url:\n",
    "\n",
    "                #Add the image url and corresponding date to the lists\n",
    "                all_urls.append(image_url)\n",
    "                all_dates.append(date_taken)\n",
    "                print(\"carousel image added\")\n",
    "\n",
    "            #Extract the video URL from the media\n",
    "            video_versions = media.get('video_versions', [])\n",
    "            if video_versions:\n",
    "                video_url = video_versions[0].get('url')\n",
    "                if video_url:\n",
    "                    #Add the video URL and corresponding date to the lists\n",
    "                    all_urls.append(video_url)\n",
    "                    all_dates.append(date_taken)\n",
    "                    print(\"carousel video added\")\n",
    "        \n",
    "        #handle cases of unique image, instead of carousel\n",
    "        image_url = item.get('image_versions2', {}).get('candidates', [])[0].get('url')\n",
    "        if image_url:\n",
    "            #Add the image URL and corresponding date to the lists\n",
    "            all_urls.append(image_url)\n",
    "            all_dates.append(date_taken)\n",
    "            print(\"single image added\")\n",
    "\n",
    "        #check if 'video_versions' key exists\n",
    "        video_versions =  item.get('video_versions', [])\n",
    "        if video_versions:\n",
    "            video_url = video_versions[0].get('url')\n",
    "            if video_url:\n",
    "                #Add the video URL and corresponding date to the lists\n",
    "                all_urls.append(video_url)\n",
    "                all_dates.append(date_taken)\n",
    "                print(\"single video added\")\n",
    "\n",
    "print(len(all_urls))\n",
    "\n",
    "#create a dataframe from the lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory to store downloaded files\n",
    "base_dir = 'scraped_data'\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "#Create the base directory for alll scapped data\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "download_dir = os.path.join(base_dir, keyword)\n",
    "\n",
    "#Create subfolders for images and videos\n",
    "image_dir = os.path.join(download_dir, \"images\")\n",
    "video_dir = os.path.join(download_dir, \"videos\")\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Initialize counters for images and videos\n",
    "image_counter = 1\n",
    "video_counter = 1\n",
    "\n",
    "#iterate through URLs in the all_urls list and download media\n",
    "for index, url in enumerate(all_urls, 0):\n",
    "    response =  requests.get(url, stream=True)\n",
    "\n",
    "    #Extract file extension from the URL\n",
    "    url_path =  urlparse(url).path\n",
    "    file_extension = os.path.splitext(url_path)[1]\n",
    "\n",
    "    #Determine the file name based on the URL\n",
    "    if file_extension.lower() in {'.jpg', '.jpeg', '.png', '.gof'}:\n",
    "        file_name = f\"{all_dates[index]}-img-{image_counter}.png\"\n",
    "        destination_folder = image_dir\n",
    "        image_counter += 1\n",
    "    elif file_extension.lower() in {'.mp4', '.avi', '.mkv', '.mov'}:\n",
    "        file_name = f\"{all_dates[index]}-img-{video_counter}.mp4\"\n",
    "        destination_folder = video_dir\n",
    "        video_counter += 1\n",
    "    else:\n",
    "        #Default to the main download directory for other file types\n",
    "        file_name = f\"{all_dates[index]}{file_extension}\"\n",
    "        destination_folder = download_dir\n",
    "\n",
    "    #Save the file to the appropriate folder\n",
    "    file_path = os.path.join(destination_folder, file_name)\n",
    "\n",
    "    #Write the content of the response to the file\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "\n",
    "    print(f'Downloaded: {file_path}')\n",
    "\n",
    "#Print a message indicating the number of downloaded files and the download directory\n",
    "print(f'Download {len(all_urls)} files to {download_dir}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
